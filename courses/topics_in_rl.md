---
layout: page
title: Topics in Reinforcement Learning
permalink: /teaching/topics-in-rl
---

Reinforcement learning (RL) is a powerful framework for solving sequential decision making problems
and has enjoyed tremendous success, e.g., [playing the game of Go](https://www.nature.com/articles/nature16961) and [nuclear fusion control](https://www.nature.com/articles/s41586-021-04301-9).


In this course,
we will dive into some theoretical topics of RL.
You are expected to be comfortable with reading and writing proofs involving linear algebra and probability. 
You are **NOT** expected to know RL. 
We will make sure you can catch up even if you do not know RL before.  

After this course, you will be able to catch up with most RL papers easily and be well prepared to do research in RL.

## Logistics:

- Instructor: [Shangtong Zhang](/)
- Location: Mechanical Engineering Building 339   
- Time: Tuesday and Thursday 9:30am - 10:45am  
- TA: TBA
- Discussion: TBA
- Office Hour: TBA

## Topics:
- classification of Markov chains
- ergodicity of Markov chains
- Markov decision processes and performance metrics
- dynamic programming
- stochastic approximation 
- Monte Carlo and TD
- importance sampling
- off-policy TD and Q-learning
- linear function approximation
- asymptotic convergence of linear TD
- finite sample analysis of linear TD
- the deadly triad
- gradient TD
- emphatic TD 

## Grading (tentative, subject to change):
- Reading assignment (20%):  
There will be several reading assignments and you are required to write a short summary of the reading materials, as well as to ask some theoretical questions (you do not need to answer them).
- Project (80%):  
Each student is expected to complete a course project solo.
You will need to submit a project proposal (10%) and a final writeup and delivery a project presentation in class.
You can choose one of the following two types of projects:
  - Reproduce the proofs of a theoretical paper from this list  
  30% writeup + 40% presentation. You can also choose a paper not from this list but make sure to talk with me first.
  - Investigate a research idea from this list (it might be empirical and/or theoretical)  
  50% writeup + 20% presentation. You can also propose a research idea by yourself but make sure to talk with me first.

## Schedule (tentative, subject to change):

| Date  | Lecture | Comments |
|-------|---------|----------|
| 08/23 | |          |
| 08/25 | |          |
| 08/30 | |          |
| 09/01 |         |          |
| 09/06 |         |          |
| 09/08 |         |          |
| 09/13 |         |          |
| 09/15 |         |          |
| 09/20 |         |          |
| 09/22 |         |          |
| 09/27 |         |          |
| 09/29 |         |          |
| 10/04 | No class| Reading days |
| 10/06 |         |          |
| 10/11 |         |          |
| 10/13 |         |          |
| 10/18 |         |          |
| 10/20 |         |          |
| 10/25 |         |          |
| 10/27 |         |          |
| 11/01 |         |          |
| 11/03 |         |          |
| 11/08 | No class|  Election day|
| 11/10 |         |          |
| 11/15 |         |          |
| 11/17 |         |          |
| 11/22 |         |          |
| 11/24 | No class | Thanksgiving recess|
| 11/29 |         |          |
| 12/01 |         |          |
| 12/06 |         |          |

## Resources:
- [*Reinforcement Learning: An Introduction*](http://incompleteideas.net/book/the-book-2nd.html) by Richard Sutton and Andrew Barto
- *Markov Decision Processes: Discrete Stochastic Dynamic Programming* by Martin Puterman
- *Stochastic Approximation: A Dynamical Systems Viewpoint* by Vivek Borkar
- *Neuro-Dynamic Programming* by Dimitri Bertsekas
- [*Safe Reinforcement Learning*](https://scholarworks.umass.edu/dissertations_2/514/) by Philip Thomas
- *Breaking the Deadly Triad in Reinforcement Learning* by Shangtong Zhang