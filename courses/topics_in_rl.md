---
layout: page
title: Topics in Reinforcement Learning
permalink: /teaching/topics-in-rl
---

Reinforcement learning (RL) is a powerful framework for solving sequential decision making problems
and has enjoyed tremendous success, e.g., [playing the game of Go](https://www.nature.com/articles/nature16961) and [nuclear fusion control](https://www.nature.com/articles/s41586-021-04301-9).


In this course,
we will dive into some theoretical topics of RL,
such as
* fundamentals of Markov Decision Processes
* off-policy learning with function approximation
* asymptotic and non-asymptotic analysis of representative RL algorithms

You are expected to be comfortable with reading and writing proofs involving linear algebra and probability. 
You are **NOT** expected to know RL. 
We will make sure you can catch up even if you do not know RL before.  

After this course, you will be able to catch up with most RL papers easily and be well prepared to do research in RL.

- Instructor: [Shangtong Zhang](/)
- Location: Mechanical Engineering Building 339   
- Time: Tuesday and Thursday 9:30am - 10:45am  
- TA: TBA
- Discussion: TBA
- Office Hour: TBA

## Grading (tentative, subject to change):
- Reading assignment (20%):  
There will be several reading assignments and you are required to write a short summary of the reading materials, as well as to ask some theoretical questions (you do not need to answer them).
<!-- - Paper presentation (20%):   -->
<!-- Each student is expected to present a paper focusing on the theoretical results of the paper. -->
- Project (80%):  
Each student is expected to complete a course project solo.
You will need to submit a project proposal (10%) and a final writeup and delivery a project presentation in class.
You can choose one of the following three types of projects:
  - Reproduce the proofs of a theoretical paper from this list  
  30% writeup + 40% presentation
  - Investigate a research idea from this list (it might be empirical and/or theoretical)  
  50% writeup + 20% presentation
  - Propose a new research project by yourself (talk with me first if you choose this one)  
  50% writeup + 20% presentation

## Schedule (tentative, subject to change):

Topics:
- classification of Markov chains
- ergodicity of Markov chains
- Markov decision processes and performance metrics
- dynamic programming
- stochastic approximation 
- Monte Carlo and TD
- importance sampling
- off-policy TD and Q-learning
- linear function approximation
- asymptotic convergence of linear TD
- finite sample analysis of linear TD
- the deadly triad
- gradient TD
- emphatic TD 

| Date  | Lecture | Comments |
|-------|---------|----------|
| 08/23 | |          |
| 08/25 | |          |
| 08/30 | |          |
| 09/01 |         |          |
| 09/06 |         |          |
| 09/08 |         |          |
| 09/13 |         |          |
| 09/15 |         |          |
| 09/20 |         |          |
| 09/22 |         |          |
| 09/27 |         |          |
| 09/29 |         |          |
| 10/04 | No class| Reading days |
| 10/06 |         |          |
| 10/11 |         |          |
| 10/13 |         |          |
| 10/18 |         |          |
| 10/20 |         |          |
| 10/25 |         |          |
| 10/27 |         |          |
| 11/01 |         |          |
| 11/03 |         |          |
| 11/08 | No class|  Election day|
| 11/10 |         |          |
| 11/15 |         |          |
| 11/17 |         |          |
| 11/22 |         |          |
| 11/24 | No class | Thanksgiving recess|
| 11/29 |         |          |
| 12/01 |         |          |
| 12/06 |         |          |

## Resources:
### Books:
- [Reinforcement Learning: An Introduction](http://incompleteideas.net/book/the-book-2nd.html)
- [Markov Decision Processes: Discrete Stochastic Dynamic Programming](https://www.amazon.ca/Markov-Decision-Processes-Stochastic-Programming/dp/0471727822)
- [Stochastic Approximation: A Dynamical Systems Viewpoint](https://www.amazon.com/Stochastic-Approximation-Dynamical-Systems-Viewpoint/dp/0521515920)

### Papers (TBA):

### Theses (TBA):