---
layout: page
title: Optimization 
permalink: /teaching/cs4501_fall_23/index
---

The recent success in deep learning is largely driven by the application of large neural networks.
In this course,
we will cover the foundations of the techniques that are commonly used to train those large neural networks,
namely,
the foundations of the first-order optimization methods.
Those include gradient descent and its variants.
This course will cover both theoretical analysis and empirical implementation.

WARNING: This course is mathematically heavy but does involve a good amount of coding.

## Logistics:

- Instructor: [Shangtong Zhang](/)
- Location: TBD   
- Time: Tuesday & Thursday, 14:00 - 15:15  
- TA: TBD 
- Office Hour: 
  - Shangtong: Tuesday & Thursday 15:30 - 16:30 (Rice Hall 422)
- UVACollab: [OPT-CS4501-F23]()
- Prerequisite:
  - CS 2150 or CS 3140 or equivalent: C- or better, we will use Python
  - Math 3350 or APMA 3080 or equivalent: recommended but not necessary, we will heavily use linear algebra and I will cover some related topics at the first few lectures
  - APMA 3100 or APMA 3110 or MATH 3100 or equivalent: **not** required, we will not involve probability
- Teaching: We will use the textbook [First-Order Methods in Optimization](https://epubs.siam.org/doi/10.1137/1.9781611974997) by Amir Beck. I will mostly use white boards instead of slides. 
I will **NOT** provide notes.
As a courtesy,
I will try my best to record each lecture and post the recording in Collab.
<!-- It is ok to not have the textbook. My whiteboards will be self-contained. -->

## Roadmap:
We will cover four algorithms: gradient descent, projected gradient descent, mirror descent, and proximal gradient descent.
Chapters 1 - 7 provide background for the whole textbook.
We will only cover the background related to the four algorithms.
Chapters 8 - 10 analyze the four algorithms in depth.
We will only cover their basic forms.
The entire book rests on the idea of subgradient.
We will cover only its special case, gradient,
which simplifies everything.

- Gradient Descent
  - Basics of Vector Space  
    - Definition, Dimension, Norm, Inner Products, Convex Sets, Euclidean Spaces
    - **Written Assignment 1** (14%)
  - Linear Transformation
  - Dual Space
  - Closedness
  - Convex Function
  - Gradients
  - Smoothness
  - **Written Assignment 2** (14%)
  - Descent Direction  
    - Lem 8.2
- Projected Gradient Descent  
  - Lem 8.11, Thm 8.13, Thm 8.16, Thm 8.17, Thm 8.18, Thm 8.30, Thm 8.31, Thm 8.33
  - **Coding Assignment 1** (14%): Implementation of Projected Gradient Descent
- Mirror Descent
  - Bregman Distance
    - Lem 9.4
  - Lem 9.11, Lem 9.12, Lem 9.13, Lem 9.14
  - **Coding Assignment 2** (14%): Implementation of Mirror Descent 
- Proximal Gradient Descent
  - Proximal Operator
  - Convex Case: Sec 10.4
  - **Coding Assignment 3** (14%): Implementation of Proximal Gradient Descent 
- Final Project
  - **Written Part** (5%): Derivation of the chain rule of the feedforward neural network 
  - **Coding Part** (25%): Implementation of the chain rule of the feedforward neural network

All the written assignments are expected to be **PDF files** generated by LaTeX. 
[Here](/blog/latex) are some tips for LaTeX.
The final letter grade is based on curved scores.

## Key Dates:
Oct 3: Reading days, no lecture  
Nov 7: Election day, no lecture  
Nov 23: Thanksgiving recess, no lecture

<!-- | Date  |  Comments |
|-------| ----------|
| 08/22 |   |
| 08/24 |  | 
| 08/29 |  |
| 08/31 |  | 
| 09/05 |  | 
| 09/07 | |
| 09/12 |  |        
| 09/14 |            |
| 09/19 |  |          
| 09/21 |            |
| 09/26 | |
| 09/28 |                   |
| 10/03 |  Reading Days |
| 10/05 | |
| 10/10 |   |
| 10/12 |  |
| 10/17 |                   |
| 10/19 |   |
| 10/24 | |
| 10/26 |  |
| 10/31 |                   |
| 11/02 |                   |
| 11/07 | Election Day |
| 11/09 |  |
| 11/14 |  |
| 11/16 |  |
| 11/21 |  |
| 11/23 | Thanksgiving recess |
| 11/28 | |
| 11/30 | |
| 12/05 | | -->

## Policies:

<!-- No late submission is allowed except for medical needs and reasonable career development needs. -->
<!-- See all policies [here](/teaching/policies). -->

- Late Policy:
If you submit within 8 hours (a grace period) after the due date,
there is no penalty.
If you submit within 24 hours after the due date, you lose 33% scores.
If you submit within 48 hours after the due date, you lose 66% scores.
If you are not able to submit within 48 hours after the due date, 
you lose all scores.
No exception will be made unless a doctor's note is provided.
- Regrading Policy: For every assignment, one regrading request is allowed. I will regrade the **entire** assignment and there is no guarantee that the score will not decrease.