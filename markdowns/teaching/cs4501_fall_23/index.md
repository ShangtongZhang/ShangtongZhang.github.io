---
layout: page
title: Optimization 
permalink: /teaching/cs4501_fall_23/index
---

The recent success in deep learning is largely driven by the application of large neural networks.
In this course,
we will cover the foundations of the techniques that are commonly used to train those large neural networks,
namely,
the foundations of the first-order optimization methods.
Those include gradient descent and its variants.
This course will cover both theoretical analysis and empirical implementation.

WARNING: This course is mathematically heavy but does involve a good amount of coding.

## Logistics:

- Instructor: [Shangtong Zhang](/)
- Location: Olsson Hall 120   
- Time: Tuesday & Thursday, 14:00 - 15:15  
- TA: TBD 
- Office Hour: 
  - Shangtong: Tuesday & Thursday 15:30 - 16:30 (Rice Hall 422)
- UVACanvas: [23F CS4501 Optimization](https://canvas.its.virginia.edu/courses/79226)
- Prerequisite:
  - APAM 2120 or equivalent: C- or better, we will use calculus
  - CS 2150 or CS 3140 or equivalent: C- or better, we will use Python
  - Math 3350 or APMA 3080 or equivalent: recommended but not necessary, we will heavily use linear algebra and I will cover some related topics at the first few lectures
  - APMA 3100 or APMA 3110 or MATH 3100 or equivalent: **not** required, we will not involve probability
- Teaching: We will use the textbook [First-Order Methods in Optimization](https://epubs.siam.org/doi/10.1137/1.9781611974997) (FOMO) by Amir Beck.
We will also use [Algebra, Topology, Differential Calculus, and  Optimization Theory for Computer Science and Machine Learning](https://www.cis.upenn.edu/~jean/gbooks/geomath.html) (ATDO) by Jean Gallier and Jocelyn Quaintance as reference.
FOMO is not free but ATDO is free.
There will be no slides and the lectures are mostly whiteboards.
In principle, if you attend each lecture and take notes carefully, purchasing FOMO is not necessary.
That being said,
I will **NOT** provide detailed notes.
I will, however, provide an outline.pdf which is used to remind myself of what to discuss next.
As a courtesy,
I will try my best to record each lecture (not guaranteed) and post the recording in Canvas.

## Roadmap:
We will cover four algorithms: gradient descent, projected gradient descent, mirror descent, and proximal gradient descent.
Chapters 1 - 7 provide background for the whole textbook.
We will only cover the background related to the four algorithms.
Chapters 8 - 10 analyze the four algorithms in depth.
We will only cover their basic forms.
The entire book rests on the idea of subgradient.
We will cover only its special case, gradient,
which simplifies everything.

- Introduction
  - Newton's method
    - **Assignment 1** (10%): Implementation of Newton's method
- Gradient Descent
  - Basics of Vector Space  
    - Definition, Dimension, Norm, Inner Products, Convex Sets, Euclidean Spaces
    - **Assignment 2** (15%): Math basics
  - Linear Transformation
  - Dual Space
  - Closedness
  - Convex Function
  - Gradients
  - Smoothness
  - **Assignment 3** (15%): Math basics
  - Descent Direction  
    - Lem 8.2
- Projected Gradient Descent  
  - Lem 8.11, Thm 8.13, Thm 8.16, Thm 8.17, Thm 8.18, Thm 8.30, Thm 8.31, Thm 8.33
  - **Assignment 4** (10%): Implementation of Projected Gradient Descent
- Mirror Descent
  - Bregman Distance
    - Lem 9.4
  - Lem 9.11, Lem 9.12, Lem 9.13, Lem 9.14
  - **Assignment 5** (10%): Implementation of Mirror Descent 
- Proximal Gradient Descent
  - Proximal Operator
  - Convex Case: Sec 10.4
  - **Assignment 6** (10%): Implementation of Proximal Gradient Descent 
- **Assignment 7 (final project)** 
  - **Written Part** (5%): Derivation of the chain rule of the feedforward neural network 
  - **Coding Part** (25%): Implementation of the chain rule of the feedforward neural network

All the written assignments are expected to be **PDF files** generated by LaTeX. 
[Here](/blog/latex) are some tips for LaTeX.
The final letter grade is based on curved scores.

<!-- ## Key Dates:
Oct 3: Reading days, no lecture  
Nov 7: Election day, no lecture  
Nov 23: Thanksgiving recess, no lecture -->

| Date  |  Comments |
|-------| ----------|
| 08/22 |  [Assignment 1](https://github.com/ShangtongZhang/ShangtongZhang.github.io/tree/master/assets/pdf/cs_4501_fall_23) released. |
| 08/24 |  | 
| 08/29 |  |
| 08/31 |  | 
| 09/05 |  | 
| 09/07 | |
| 09/12 |  |        
| 09/14 |            |
| 09/19 |  Assignment 1 due. Assignment 2 released.|          
| 09/21 |            |
| 09/26 | |
| 09/28 |                   |
| 10/03 |  Reading Days, no lecture. Assignment 2 due. Assignment 3 released. |
| 10/05 | |
| 10/10 |   |
| 10/12 |  |
| 10/17 |  Assignment 3 due. Assignment 4 released.                 |
| 10/19 |   |
| 10/24 | |
| 10/26 |  |
| 10/31 |  Assignment 4 due. Assignment 5 released.                 |
| 11/02 |                   |
| 11/07 | Election Day, no lecture  |
| 11/09 |  |
| 11/14 | Assignment 5 due. Assignments 6 & 7 released. |
| 11/16 |  |
| 11/21 |  |
| 11/23 | Thanksgiving recess, no lecture  |
| 11/28 | Assignment 6 due. |
| 11/30 | |
| 12/05 | Last lecture. |
| 12/10 | Assignment 7 due.|

## Policies:

- Late Policy:
If you submit within 8 hours (a grace period) after the due date,
there is no penalty.
If you submit within 24 hours after the due date, you lose 33% scores.
If you submit within 48 hours after the due date, you lose 66% scores.
If you are not able to submit within 48 hours after the due date, 
you lose all scores.
No exception will be made unless a doctor's note is provided.
- Regrading Policy: For every assignment, one regrading request is allowed. I will regrade the **entire** assignment and there is no guarantee that the score will not decrease.