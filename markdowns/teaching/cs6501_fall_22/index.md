---
layout: page
title: Topics in Reinforcement Learning
permalink: /teaching/cs6501_fall_22/index
---

Reinforcement learning (RL) is a powerful framework for solving sequential decision making problems
and has enjoyed tremendous success, e.g., [playing the game of Go](https://www.nature.com/articles/nature16961) and [nuclear fusion control](https://www.nature.com/articles/s41586-021-04301-9).

In this course,
we will dive into some theoretical topics of RL.
You are expected to be comfortable with reading and writing proofs involving linear algebra and probability. 
You are **NOT** expected to know RL. 
We will make sure you can catch up even if you do not know RL before.  

After this course, you will be able to catch up with most RL papers easily and be well prepared to do research in RL.

## Logistics:

- Instructor: [Shangtong Zhang](/)
- Location: Mechanical Engineering Building 339   
- Time: Tuesday and Thursday 9:30am - 10:45am  
- TA: TBA
- Discussion: TBA
- Office Hour: TBA

## Topics:
- classification of Markov chains
- ergodicity of Markov chains
- Markov decision processes and performance metrics
- dynamic programming
- stochastic approximation 
- Monte Carlo and TD
- importance sampling
- off-policy TD and Q-learning
- linear function approximation
- asymptotic convergence of linear TD
- finite sample analysis of linear TD
- the deadly triad
- gradient TD
- emphatic TD 

## Grading (tentative, subject to change):
- [Reading assignment](/teaching/cs6501_fall_22/reading) (5% x 6 = 30%):  
- [A Research or Non-research Course Project](/teaching/cs6501_fall_22/projects) (70%):  
  * Project proposal (5%)
  * Progress report (5%)
  * Write-up (30%)
  * Presentation (30%): The length of the presentation will be decided later depending on the number of groups but is at most 35 minutes including questions.

You have the choice, before the due of the first reading assignment, to redistribute the 30% of the reading assignment to the **research** project.
Then you will have reading assignment (0%), write-up (45%), and presentation (45%).
Consequently, I will use a higher standard when evaluating your research project.
Though it is totally up to you,
I recommend considering this only if you are already familiar with Sutton and Barto's book.
Please email me for confirmation if you want to do so.

All the submissions are expected to be **PDF files** generated by LaTeX. 
[Here](/blog/latex) are some tips for LaTeX.
[Here](/blog/writing) are some tips for writing.

## Schedule (tentative, subject to change):

| Date  |  Comments |
|-------| ----------|
| 08/23 |   |
| 08/25 |           | 
| 08/30 |  |
| 09/01 |  | 
| 09/06 |  | 
| 09/08 |  |
| 09/13 |  1st reading assignment due; project proposal due|        
| 09/15 |            |
| 09/20 |  2nd reading assignment due|          
| 09/22 |            |
| 09/27 |  3rd reading assignment due|
| 09/29 |                   |
| 10/04 | **No class, reading days** |
| 10/06 |  4th reading assignment due                 |
| 10/11 |                   |
| 10/13 |  5th reading assignment due; progress report due |
| 10/18 |                   |
| 10/20 |  6th reading assignment due               |
| 10/25 |                   |
| 10/27 |                   |
| 11/01 |                   |
| 11/03 |                   |
| 11/08 | **No class, election day** |
| 11/10 |                   |
| 11/15 |                   |
| 11/17 |                   |
| 11/22 |                   |
| 11/24 | **No class, Thanksgiving recess**|
| 11/29 |                   |
| 12/01 |                   |
| 12/06 | Writeup due                   |

## Resources:
Books:
- [*Reinforcement Learning: An Introduction*](http://incompleteideas.net/book/the-book-2nd.html) by Richard Sutton and Andrew Barto
- *Markov Decision Processes: Discrete Stochastic Dynamic Programming* by Martin Puterman
- *Stochastic Approximation: A Dynamical Systems Viewpoint* by Vivek Borkar
- *Neuro-Dynamic Programming* by Dimitri Bertsekas

Theses:
- [*Safe Reinforcement Learning*](https://scholarworks.umass.edu/dissertations_2/514/) by Philip Thomas
- [*Breaking the Deadly Triad in Reinforcement Learning*](/assets/pdf/thesis.pdf) by Shangtong Zhang
- [*Actor-Critic Algorithms*](https://dspace.mit.edu/bitstream/handle/1721.1/8120/51552606-MIT.pdf;sequence=2) by Vijaymohan Konda