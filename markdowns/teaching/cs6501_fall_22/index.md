---
layout: page
title: Topics in Reinforcement Learning
permalink: /teaching/cs6501_fall_22/index
---

Reinforcement learning (RL) is a powerful framework for solving sequential decision making problems
and has enjoyed tremendous success, e.g., [playing the game of Go](https://www.nature.com/articles/nature16961) and [nuclear fusion control](https://www.nature.com/articles/s41586-021-04301-9).

In this course,
we will dive into some theoretical topics of RL.
You are expected to be comfortable with reading and writing proofs involving linear algebra and probability. 
You are **NOT** expected to know RL. 
We will make sure you can catch up even if you do not know RL before.  

After this course, you will be able to catch up with most RL papers easily and be well prepared to do research in RL.

## Logistics:

- Instructor: [Shangtong Zhang](/)
- Location: Mechanical Engineering Building 339   
- Time: Tuesday & Thursday, 9:30 - 10:45  
- TA: [Chijung Jung](https://chijung-jung.github.io)
- Office Hour: 
  - Shangtong: Tuesday & Thursday 11:00 - 12:00 (422 Rice Hall)
  - Chijung: Tuesday 13:00 - 15:00 ([Zoom](https://virginia.zoom.us/j/5944735371) by default, or in-persion by request)
- UVACollab: [RL-CS6501-F22](https://collab.its.virginia.edu/portal/site/ff8396e5-8111-44cc-ade2-8311272db7eb/tool/74aa2784-e9cf-41b8-bec6-5fef35f79093)


## Topics:
- classification of Markov chains
- ergodicity of Markov chains
- Markov decision processes and performance metrics
- dynamic programming
- stochastic approximation 
- Monte Carlo and TD
- importance sampling
- off-policy TD and Q-learning
- linear function approximation
- asymptotic convergence of linear TD

## Grading (tentative):
- [Reading assignment](/teaching/cs6501_fall_22/reading) (5% x 6 = 30%):  
- [A Research or Non-research Course Project](/teaching/cs6501_fall_22/projects) (70%):  
  * Project proposal (5%)
  * Progress report (5%)
  * Write-up (30%)
  * Presentation (30%): The length of the presentation will be decided later depending on the number of groups but is at most 35 minutes including questions.

You have the choice, before the due of the first reading assignment, to redistribute the 30% of the reading assignment to the **research** project.
Then you will have reading assignment (0%), write-up (45%), and presentation (45%).
Consequently, I will use a higher standard when evaluating your research project.
Though it is totally up to you,
I recommend considering this only if you are already familiar with Sutton and Barto's book.
Please email me for confirmation if you want to do so.

All the submissions are expected to be **PDF files** generated by LaTeX. 
[Here](/blog/latex) are some tips for LaTeX.
[Here](/blog/writing) are some tips for writing.

## Schedule:

| Date  |  Comments |
|-------| ----------|
| 08/23 | [introduction.pdf](/assets/pdf/cs_6501_fall_22/introduction.pdf)  |
| 08/25 |  [markov_chains.pdf](/assets/pdf/cs_6501_fall_22/markov_chains.pdf) (Revised on Sept 6)| 
| 08/30 |  |
| 09/01 |  | 
| 09/06 |  | 
| 09/08 | [markov_decision_process.pdf](/assets/pdf/cs_6501_fall_22/markov_decision_process.pdf) (Revised on Sept 26)|
| 09/13 |  1st reading assignment due |        
| 09/15 |            |
| 09/20 |  2nd reading assignment due; project proposal due|          
| 09/22 |            |
| 09/27 | [dynamic_programming.pdf](/assets/pdf/cs_6501_fall_22/dynamic_programming.pdf) (Revised on Oct 6); 3rd reading assignment due|
| 09/29 |                   |
| 10/04 | **No class, reading days** |
| 10/06 |  4th reading assignment due                 |
| 10/11 |  [temporal_difference_learning.pdf](/assets/pdf/cs_6501_fall_22/temporal_difference_learning.pdf)                 |
| 10/13 |  5th reading assignment due|
| 10/18 |                   |
| 10/20 |  6th reading assignment due |
| 10/25 |                   |
| 10/27 | progress report due |
| 11/01 |                   |
| 11/03 |                   |
| 11/08 | **No class, election day** |
| 11/10 | Project Presentation                  |
| 11/15 | Project Presentation                 |
| 11/17 | Project Presentation                  |
| 11/22 | Project Presentation                   |
| 11/24 | **No class, Thanksgiving recess**|
| 11/29 | **No class, NeurIPS** (tentative)                  |
| 12/01 | **No class, NeurIPS** (tentative)                |
| 12/06 | Project Presentation                   |
| 12/16 | Writeup due                   |

## Resources:
Books:
- [*Reinforcement Learning: An Introduction*](http://incompleteideas.net/book/the-book-2nd.html) by Richard Sutton and Andrew Barto
- [*Markov Decision Processes: Discrete Stochastic Dynamic Programming*](https://onlinelibrary-wiley-com.proxy01.its.virginia.edu/doi/book/10.1002/9780470316887) by Martin Puterman
- *Stochastic Approximation: A Dynamical Systems Viewpoint* by Vivek Borkar
- *Neuro-Dynamic Programming* by Dimitri Bertsekas and John Tsitsiklis 
- [*Markov Chains and Mixing Times*](https://pages.uoregon.edu/dlevin/MARKOV/markovmixing.pdf) by David Asher Levin, Elizabeth Wilmer, and Yuval Peres

Theses:
- [*Safe Reinforcement Learning*](https://scholarworks.umass.edu/dissertations_2/514/) by Philip Thomas
- [*Breaking the Deadly Triad in Reinforcement Learning*](https://ora.ox.ac.uk/objects/uuid:2c410803-2141-41ed-b362-7f14723b2f17) by Shangtong Zhang
- [*Actor-Critic Algorithms*](https://dspace.mit.edu/bitstream/handle/1721.1/8120/51552606-MIT.pdf;sequence=2) by Vijaymohan Konda

Notes:
- [*Introduction to discrete-time Markov chains I*](http://www.columbia.edu/~ks20/stochastic-I/stochastic-I-MCI.pdf) by Karl Sigman
- [*Markov chains II: recurrence and limiting (stationary) distributions*](http://www.columbia.edu/~ks20/stochastic-I/stochastic-I-MCII.pdf) by Karl Sigman

## Policies:

No late submission is allowed except for medical needs and reasonable career development needs.
See all policies [here](/teaching/policies).